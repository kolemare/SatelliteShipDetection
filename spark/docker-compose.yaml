services:
  postgres:
    image: postgres:16
    container_name: spark_pg
    environment:
      POSTGRES_DB: aoi
      POSTGRES_USER: aoi
      POSTGRES_PASSWORD: aoi
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./postgres-init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U aoi -d aoi"]
      interval: 5s
      timeout: 5s
      retries: 10

  minio:
    image: minio/minio:latest
    container_name: spark_minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 5s
      retries: 20

  minio-init:
    image: minio/mc:latest
    container_name: spark_minio_init
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MC_HOST_minio: "http://minio:minio123@minio:9000"
    entrypoint: >
      /bin/sh -lc "
      mc mb --ignore-existing minio/aoi &&
      mc ls minio
      "
    restart: "no"

  redpanda:
    image: redpandadata/redpanda:latest
    container_name: spark_redpanda
    command:
      - redpanda
      - start
      - --overprovisioned
      - --smp=1
      - --memory=1G
      - --reserve-memory=0M
      - --node-id=0
      - --kafka-addr=PLAINTEXT://0.0.0.0:9092,OUTSIDE://0.0.0.0:19092
      - --advertise-kafka-addr=PLAINTEXT://redpanda:9092,OUTSIDE://localhost:19092
    ports:
      - "9092:9092"
      - "19092:19092"
      - "9644:9644"
    volumes:
      - redpanda:/var/lib/redpanda/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9644/v1/status/ready"]
      interval: 5s
      timeout: 5s
      retries: 30

  # (one-shot) Create topic "aoi_jobs" after Redpanda is healthy
  redpanda-init:
    image: redpandadata/redpanda:latest
    container_name: spark_redpanda_init
    depends_on:
      redpanda:
        condition: service_healthy
    entrypoint: >
      /bin/sh -lc "
      rpk cluster info --brokers redpanda:9092 >/dev/null 2>&1 ||
      { echo 'Redpanda not ready'; exit 1; };
      rpk topic create aoi_jobs --brokers redpanda:9092 --if-not-exists;
      rpk topic list --brokers redpanda:9092
      "
    restart: "no"

  # ---------- Build the image ONCE, then reuse ----------
  spark-base:
    build:
      context: ./app
      dockerfile: Dockerfile
    image: spark-spark-app
    container_name: spark_base
    command: ["bash", "-lc", "sleep infinity"]
    # This service just sits there so the image exists locally.

  spark-master:
    image: spark-spark-app
    container_name: spark_master
    depends_on:
      spark-base:
        condition: service_started
    environment:
      - SPARK_NO_DAEMONIZE=true
    command: ["/opt/spark/bin/spark-class",
              "org.apache.spark.deploy.master.Master",
              "--host", "spark-master",
              "--port", "7077",
              "--webui-port", "8080"]
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - spark-events:/opt/spark-events

  spark-worker-1:
    image: spark-spark-app
    container_name: spark_worker_1
    depends_on:
      spark-master:
        condition: service_started
      spark-base:
        condition: service_started
    environment:
      - SPARK_NO_DAEMONIZE=true
    command: ["/opt/spark/bin/spark-class",
              "org.apache.spark.deploy.worker.Worker",
              "spark://spark-master:7077",
              "--webui-port", "8081"]
    ports:
      - "8081:8081"
    volumes:
      - spark-events:/opt/spark-events

  spark-worker-2:
    image: spark-spark-app
    container_name: spark_worker_2
    depends_on:
      spark-master:
        condition: service_started
      spark-base:
        condition: service_started
    environment:
      - SPARK_NO_DAEMONIZE=true
    command: ["/opt/spark/bin/spark-class",
              "org.apache.spark.deploy.worker.Worker",
              "spark://spark-master:7077",
              "--webui-port", "8082"]
    ports:
      - "8082:8082"
    volumes:
      - spark-events:/opt/spark-events

  spark-worker-3:
    image: spark-spark-app
    container_name: spark_worker_3
    depends_on:
      spark-master:
        condition: service_started
      spark-base:
        condition: service_started
    environment:
      - SPARK_NO_DAEMONIZE=true
    command: ["/opt/spark/bin/spark-class",
              "org.apache.spark.deploy.worker.Worker",
              "spark://spark-master:7077",
              "--webui-port", "8083"]
    ports:
      - "8083:8083"
    volumes:
      - spark-events:/opt/spark-events

  spark-history:
    image: spark-spark-app
    container_name: spark_history
    depends_on:
      spark-master:
        condition: service_started
      spark-base:
        condition: service_started
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/opt/spark-events
    command: ["/opt/spark/bin/spark-class",
              "org.apache.spark.deploy.history.HistoryServer"]
    ports:
      - "18080:18080"
    volumes:
      - spark-events:/opt/spark-events

  spark-app:
    image: spark-spark-app
    container_name: spark_app
    depends_on:
      spark-master: { condition: service_started }
      spark-worker-1: { condition: service_started }
      spark-worker-2: { condition: service_started }
      spark-worker-3: { condition: service_started }
      redpanda: { condition: service_healthy }
      redpanda-init: { condition: service_completed_successfully }
      postgres: { condition: service_healthy }
      minio: { condition: service_healthy }
      minio-init: { condition: service_completed_successfully }
    environment:
      - KAFKA_BROKER=redpanda:9092
      - KAFKA_TOPIC=aoi_jobs
      - KAFKA_GROUP=aoi_processor
      - DB_URL=postgresql+psycopg2://aoi:aoi@postgres:5432/aoi
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - S3_BUCKET=aoi
      - SPARK_MASTER=${SPARK_MASTER}
      - APP_MAIN=${APP_MAIN:-/opt/app/main.py}
    command:
      - /opt/spark/bin/spark-submit
      - --master
      - spark://spark-master:7077
      - --verbose
      - --conf
      - spark.eventLog.enabled=true
      - --conf
      - spark.eventLog.dir=/opt/spark-events
      - --conf
      - spark.history.fs.logDirectory=/opt/spark-events
      - --conf
      - spark.executor.instances=3
      - --conf
      - spark.executor.cores=1
      - --conf
      - spark.task.cpus=1
      - --conf
      - spark.dynamicAllocation.enabled=false
      - --conf
      - spark.default.parallelism=3
      - --conf
      - spark.sql.shuffle.partitions=3
      - --conf
      - spark.pyspark.python=python3
      - --conf
      - spark.pyspark.driver.python=python3
      - --conf
      - spark.python.worker.reuse=true
      - --conf
      - spark.hadoop.fs.s3a.endpoint=${S3_ENDPOINT}
      - --conf
      - spark.hadoop.fs.s3a.access.key=${S3_ACCESS_KEY}
      - --conf
      - spark.hadoop.fs.s3a.secret.key=${S3_SECRET_KEY}
      - --conf
      - spark.hadoop.fs.s3a.path.style.access=true
      - --packages
      - org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1,org.apache.hadoop:hadoop-aws:3.4.1
      - --py-files
      - /opt/app/ConvNextInference.py,/opt/app/realESRGAN.py,/opt/app/tiling.py,/opt/app/providers.py
      - /opt/app/main.py
    volumes:
      - spark-events:/opt/spark-events

  aoi-submit:
    build:
      context: ./gui
    container_name: spark_aoi_submit
    ports:
      - "8501:8501"
    environment:
      DB_URL: postgresql+psycopg2://aoi:aoi@postgres:5432/aoi
      KAFKA_BROKER: redpanda:9092
      KAFKA_TOPIC: aoi_jobs
    depends_on:
      postgres:
        condition: service_healthy
      redpanda:
        condition: service_healthy

volumes:
  pgdata:
  minio:
  redpanda:
  spark-events:
